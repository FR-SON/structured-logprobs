{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/arena-ai/structured-logprobs/blob/main/docs/notebooks/notebook.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a practical guide on how to use the structured-logprobs library alongside the OpenAI API to generate structured responses with log probability features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`structured-logprobs` is distributed on PyPI and simply installed with pip install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LEzOxBTuz17L",
    "outputId": "83170ac3-a16b-4f6e-8cc9-cdf37d4bfbc8"
   },
   "outputs": [],
   "source": [
    "!pip install structured-logprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zoq76ttC0nBS"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "from openai import OpenAI\n",
    "from openai.types import ResponseFormatJSONSchema\n",
    "\n",
    "from structured_logprobs.main import add_logprobs, add_logprobs_inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An OpenAI API key is mandatory to authenticates the access to OpenAI's API. It serves as a token necessary to initialize the OpenAI client in Python, enabling you to send requests to the API and receive responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you'll be prompted to enter your OPENAI_API_KEY securely using Python's getpass module. This ensures that your key is not hardcoded, reducing the risk of accidental exposure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2QNRWDY6-dTg",
    "outputId": "ade4e64b-3606-479a-acb7-80504e47acef"
   },
   "outputs": [],
   "source": [
    "api_key = getpass.getpass(prompt=\"Enter you OPENAI_API_KEY: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bS5uatkr0m3x"
   },
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a chat completion request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to define the JSON schema, which will be used to structure the chat request to OpenAI. This schema helps OpenAI understand exactly how the response should be formatted.\n",
    "\n",
    "Below is the example of the JSON file that defines the schema used for validating the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aZ389dNjBZ19",
    "outputId": "39543bd8-9ad2-409d-b3db-88229ef1796b"
   },
   "outputs": [],
   "source": [
    "schema_content = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"answears\",\n",
    "        \"description\": \"Response to questions in JSON format\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"capital_of_France\": {\"type\": \"string\"},\n",
    "                \"the_two_nicest_colors\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\", \"enum\": [\"red\", \"blue\", \"green\", \"yellow\", \"purple\"]},\n",
    "                },\n",
    "                \"die_shows\": {\"type\": \"integer\"},\n",
    "            },\n",
    "            \"required\": [\"capital_of_France\", \"the_two_nicest_colors\", \"die_shows\"],\n",
    "            \"additionalProperties\": False,\n",
    "        },\n",
    "        \"strict\": True,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema must be validated before being used as a parameter in the request to OpenAI to ensure it is correctly structured as a valid JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gRX19vy5ANZb"
   },
   "outputs": [],
   "source": [
    "response_schema = ResponseFormatJSONSchema.model_validate(schema_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, to create the chat completion, you must set up the model, input messages, and other parameters such as logprobs and response_format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sdvFNe2b-MLE"
   },
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"I have three questions. The first question is: What is the capital of France? \"\n",
    "                \"The second question is: Which are the two nicest colors? \"\n",
    "                \"The third question is: Can you roll a die and tell me which number comes up?\"\n",
    "            ),\n",
    "        }\n",
    "    ],\n",
    "    logprobs=True,\n",
    "    response_format=response_schema.model_dump(by_alias=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zR4JGEl6EK13",
    "outputId": "1a615388-1f4c-48df-9179-d42220918a40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-ApBLeOHlpf3mbFIgVsaMy1V4Y0aB0', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='{\"', bytes=[123, 34], logprob=-6.6306106e-06, top_logprobs=[]), ChatCompletionTokenLogprob(token='capital', bytes=[99, 97, 112, 105, 116, 97, 108], logprob=-3.0545007e-06, top_logprobs=[]), ChatCompletionTokenLogprob(token='_of', bytes=[95, 111, 102], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='_F', bytes=[95, 70], logprob=-1.9361265e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token='rance', bytes=[114, 97, 110, 99, 101], logprob=-3.1281633e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token='\":\"', bytes=[34, 58, 34], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='Paris', bytes=[80, 97, 114, 105, 115], logprob=-9.0883464e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token='\",\"', bytes=[34, 44, 34], logprob=-1.9361265e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token='the', bytes=[116, 104, 101], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='_two', bytes=[95, 116, 119, 111], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='_n', bytes=[95, 110], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='ic', bytes=[105, 99], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='est', bytes=[101, 115, 116], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='_colors', bytes=[95, 99, 111, 108, 111, 114, 115], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='\":[\"', bytes=[34, 58, 91, 34], logprob=-1.9361265e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token='blue', bytes=[98, 108, 117, 101], logprob=-0.0021420512, top_logprobs=[]), ChatCompletionTokenLogprob(token='\",\"', bytes=[34, 44, 34], logprob=-4.3202e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token='green', bytes=[103, 114, 101, 101, 110], logprob=-0.01024507, top_logprobs=[]), ChatCompletionTokenLogprob(token='\"],', bytes=[34, 93, 44], logprob=-4.3202e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token='\"', bytes=[34], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='die', bytes=[100, 105, 101], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='_sh', bytes=[95, 115, 104], logprob=-4.3202e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token='ows', bytes=[111, 119, 115], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='\":', bytes=[34, 58], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token='3', bytes=[51], logprob=-1.5099844, top_logprobs=[]), ChatCompletionTokenLogprob(token='}', bytes=[125], logprob=-4.143808e-05, top_logprobs=[])], refusal=None), message=ChatCompletionMessage(content='{\"capital_of_France\":\"Paris\",\"the_two_nicest_colors\":[\"blue\",\"green\"],\"die_shows\":3}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1736761714, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_703d4ff298', usage=CompletionUsage(completion_tokens=27, prompt_tokens=133, total_tokens=160, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhance the chat completion result with log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = add_logprobs(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the original chat completion response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8YmboSB2vtXb",
    "outputId": "e7e84b1a-b1a5-461a-f992-16fac731bcaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"capital_of_France\":\"Paris\",\"the_two_nicest_colors\":[\"blue\",\"green\"],\"die_shows\":4}\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion.value.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the log_probs field, structured like the message of the response, but where values are replaced with their respective log-probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cznAG4sdEO2v",
    "outputId": "c4e53c94-3f92-4e61-e89e-fc351ae3fd87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'capital_of_France': -5.5122365e-07, 'the_two_nicest_colors': [-0.00283229711265, -0.01072703461265], 'die_shows': -0.43976903}]\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion.log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhance the chat completion result with in-line log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7KBaRghOEJKr"
   },
   "outputs": [],
   "source": [
    "chat_completion_inline = add_logprobs_inline(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method modifies the chat completion response object. The content of the message is replaced with a dictionary that includes also inline log probabilities for atomic values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0kRhM1a-MF0",
    "outputId": "c0450328-3607-4b41-d65e-c568d2a4aa25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"capital_of_France\": \"Paris\", \"capital_of_France_logprob\": -5.5122365e-07, \"the_two_nicest_colors\": [\"blue\", \"green\"], \"die_shows\": 4.0, \"die_shows_logprob\": -0.43976903}\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion_inline.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
